Transfer Function Mutation: If your nodes use different activation functions (e.g., sigmoid, ReLU, tanh), you could introduce a mutation that changes the activation function of a randomly selected node. This can introduce new non-linearities and potentially improve the network's representational power.


Topology Change - Split Node: You could split an existing node into two, with the incoming connections of the original node being randomly distributed (or duplicated with slight weight variations) to the two new nodes. The new nodes would then have their own outgoing connections (potentially inherited from the original node). This can allow for more complex feature extraction.


Softmax:
https://www.geeksforgeeks.org/how-to-implement-softmax-and-cross-entropy-in-python-and-pytorch/


MNIST data set:
https://github.com/cvdfoundation/mnist



Best hyper parameters:

batch size: 20
neuron weights max: 0.6
probablilties:
    prob1: 10000
    prob2: 10000
    prob3: 10000
start size: 11 (3 + 8)
iteration factor: 3

target1: 0.00001
target2: 0.05
