Transfer Function Mutation: If your nodes use different activation functions (e.g., sigmoid, ReLU, tanh), you could introduce a mutation that changes the activation function of a randomly selected node. This can introduce new non-linearities and potentially improve the network's representational power.


Topology Change - Split Node: You could split an existing node into two, with the incoming connections of the original node being randomly distributed (or duplicated with slight weight variations) to the two new nodes. The new nodes would then have their own outgoing connections (potentially inherited from the original node). This can allow for more complex feature extraction.


Softmax:
https://www.geeksforgeeks.org/how-to-implement-softmax-and-cross-entropy-in-python-and-pytorch/


MNIST data set:
https://github.com/cvdfoundation/mnist



Best hyper parameters:

batch size: 20
neuron weights max: 0.6
probablilties:
    prob1: 1000
    prob2: 100
    prob3: 100
start size: 10 (3 + 7)
iteration factor: 3

fitness = penalty + fitness
penalty = test_network() * 0.1

