Transfer Function Mutation: If your nodes use different activation functions (e.g., sigmoid, ReLU, tanh), you could introduce a mutation that changes the activation function of a randomly selected node. This can introduce new non-linearities and potentially improve the network's representational power.


Topology Change - Split Node: You could split an existing node into two, with the incoming connections of the original node being randomly distributed (or duplicated with slight weight variations) to the two new nodes. The new nodes would then have their own outgoing connections (potentially inherited from the original node). This can allow for more complex feature extraction.


#Softmax:
#https://www.geeksforgeeks.org/how-to-implement-softmax-and-cross-entropy-in-python-and-pytorch/


MNIST data set:
https://github.com/cvdfoundation/mnist


- try test network with 100 repeats.
- add back some operations:
    remove_neuron
    remove input connection
    remove hidden connection
    swap neurons
    remove connections where the weights are < 0.01 (autpo-pruning)


- Use categorical cross-entropy loss:
  loss_per_sample = -np.sum(y_true * np.log(y_pred_clipped), axis=1)
  mean_loss = np.mean(loss_per_sample)

  Binary Cross-Entropy (for two classes)

- try softmax for output values



